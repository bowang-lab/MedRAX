{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import warnings\n",
    "from typing import *\n",
    "import traceback\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import base64\n",
    "from PIL import Image\n",
    "import io\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Image\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langchain_core.messages import AnyMessage, HumanMessage, SystemMessage, ToolMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from transformers import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from medrax.agent import *\n",
    "from medrax.tools import *\n",
    "from medrax.utils import *\n",
    "from medrax.models import ModelFactory\n",
    "\n",
    "import json\n",
    "import openai\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from tenacity import retry, wait_exponential, stop_after_attempt\n",
    "\n",
    "# Disable verbose logging for external libraries\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.getLogger(\"httpx\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"openai\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"langchain\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"langchain_core\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"langchain_openai\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"langchain_xai\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"urllib3\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"requests\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"pinecone\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"cohere\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"datasets\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"langchain.chains.retrieval_qa\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"langchain.chains\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"langchain.retrievers\").setLevel(logging.ERROR)\n",
    "\n",
    "_ = load_dotenv()\n",
    "\n",
    "# Setup directory paths\n",
    "ROOT = \"/home/adib/MedRAX2\"\n",
    "PROMPT_FILE = f\"{ROOT}/medrax/docs/system_prompts.txt\"\n",
    "MODEL_DIR = \"/model-weights\"\n",
    "BENCHMARK_DIR = f\"{ROOT}/chestagentbench\"\n",
    "\n",
    "model_name = \"grok-4\"\n",
    "temperature = 0.7\n",
    "top_p = 1\n",
    "max_tokens = 128000\n",
    "medrax_logs = f\"{ROOT}/experiments/grok4_logs\"\n",
    "\n",
    "# Create the logs directory if it doesn't exist\n",
    "os.makedirs(medrax_logs, exist_ok=True)\n",
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tools():\n",
    "\n",
    "    rag_config = RAGConfig(\n",
    "        model=\"command-a-03-2025\",  # Chat model for generating responses\n",
    "        embedding_model=\"embed-v4.0\",  # Embedding model for the RAG system\n",
    "        rerank_model=\"rerank-v3.5\",  # Reranking model for the RAG system\n",
    "        temperature=0.3,\n",
    "        pinecone_index_name=\"medrax2\",  # Name for the Pinecone index\n",
    "        chunk_size=1500,\n",
    "        chunk_overlap=300,\n",
    "        retriever_k=7,\n",
    "        local_docs_dir=\"rag_docs\",  # Change this to the path of the documents for RAG\n",
    "        huggingface_datasets=[\"VictorLJZ/medrax2\"],  # List of HuggingFace datasets to load\n",
    "        dataset_split=\"train\",  # Which split of the datasets to use\n",
    "    )\n",
    "\n",
    "    all_tools = {\n",
    "        \"TorchXRayVisionClassifierTool\": lambda: TorchXRayVisionClassifierTool(device=device),\n",
    "        # \"ArcPlusClassifierTool\": lambda: ArcPlusClassifierTool(cache_dir=MODEL_DIR, device=device),\n",
    "        \"XRayVQATool\": lambda: XRayVQATool(cache_dir=MODEL_DIR, device=device),\n",
    "        \"ChestXRayReportGeneratorTool\": lambda: ChestXRayReportGeneratorTool(\n",
    "            cache_dir=MODEL_DIR, device=device\n",
    "        ),\n",
    "        \"XRayPhraseGroundingTool\": lambda: XRayPhraseGroundingTool(\n",
    "            cache_dir=MODEL_DIR, temp_dir=\"temp\", load_in_8bit=True, device=device\n",
    "        ),\n",
    "        # \"MedicalRAGTool\": lambda: RAGTool(config=rag_config),\n",
    "        \"WebBrowserTool\": lambda: WebBrowserTool(),\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        all_tools[\"PythonSandboxTool\"] = lambda: create_python_sandbox()\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating PythonSandboxTool: {e}\")\n",
    "        print(\"Skipping PythonSandboxTool\")\n",
    "\n",
    "    # Initialize all tools for benchmark\n",
    "    tools_dict = {}\n",
    "    for tool_name in all_tools.keys():\n",
    "        try:\n",
    "            tools_dict[tool_name] = all_tools[tool_name]()\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing {tool_name}: {e}\")\n",
    "            print(f\"Skipping {tool_name}\")\n",
    "\n",
    "    return list(tools_dict.values())\n",
    "\n",
    "\n",
    "def get_agent(tools):\n",
    "    prompts = load_prompts_from_file(PROMPT_FILE)\n",
    "    prompt = prompts[\"MEDICAL_ASSISTANT\"]\n",
    "\n",
    "    checkpointer = MemorySaver()\n",
    "\n",
    "    llm = ModelFactory.create_model(\n",
    "        model_name=model_name, temperature=temperature, top_p=top_p, max_tokens=max_tokens\n",
    "    )\n",
    "\n",
    "    agent = Agent(\n",
    "        llm,\n",
    "        tools=tools,\n",
    "        log_tools=True,\n",
    "        log_dir=\"grok4_logs\",\n",
    "        system_prompt=prompt,\n",
    "        checkpointer=checkpointer,\n",
    "    )\n",
    "    thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "    return agent, thread\n",
    "\n",
    "\n",
    "def run_medrax(agent, thread, prompt, image_urls=[]):\n",
    "    from langchain_core.messages import HumanMessage, AIMessage, AIMessageChunk\n",
    "\n",
    "    messages = [\n",
    "        HumanMessage(\n",
    "            content=[\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "            ]\n",
    "            + [{\"type\": \"image_url\", \"image_url\": {\"url\": image_url}} for image_url in image_urls]\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    final_content = \"\"\n",
    "    # Use the same streaming approach as the Gradio interface\n",
    "    for chunk in agent.workflow.stream({\"messages\": messages}, thread, stream_mode=\"updates\"):\n",
    "        for node_name, node_output in chunk.items():\n",
    "            if \"messages\" not in node_output:\n",
    "                continue\n",
    "            for msg in node_output[\"messages\"]:\n",
    "                if isinstance(msg, AIMessageChunk) and msg.content:\n",
    "                    final_content += msg.content\n",
    "                elif isinstance(msg, AIMessage) and msg.content:\n",
    "                    # A full AIMessage can sometimes be part of the stream\n",
    "                    final_content = msg.content\n",
    "\n",
    "    # After the stream is finished, get the final state for logging\n",
    "    final_state = agent.workflow.get_state(thread)\n",
    "\n",
    "    # If we successfully captured content from the stream, return it\n",
    "    if final_content:\n",
    "        return final_content.strip(), str(final_state)\n",
    "\n",
    "    # Fallback: If streaming produced no text, check the final state directly\n",
    "    # This maintains the logic of your original function as a backup.\n",
    "    for msg in reversed(final_state.get(\"messages\", [])):\n",
    "        if isinstance(msg, AIMessage) and msg.content:\n",
    "            return msg.content.strip(), str(final_state)\n",
    "\n",
    "    # If no content was found anywhere, report it.\n",
    "    return \"No AI response found\", str(final_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multimodal_request(question_data, case_details, case_id, question_id, agent, thread):\n",
    "\n",
    "    # Parse required figures - simplified to work with actual data structure\n",
    "    try:\n",
    "        # Get image paths from question data\n",
    "        if isinstance(question_data[\"images\"], str):\n",
    "            try:\n",
    "                required_figures = json.loads(question_data[\"images\"])\n",
    "            except json.JSONDecodeError:\n",
    "                required_figures = [question_data[\"images\"]]\n",
    "        elif isinstance(question_data[\"images\"], list):\n",
    "            required_figures = question_data[\"images\"]\n",
    "        else:\n",
    "            required_figures = [str(question_data[\"images\"])]\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing figures: {e}\")\n",
    "        required_figures = []\n",
    "\n",
    "    # Load local images and convert to base64\n",
    "    image_data_urls = []\n",
    "    valid_figures = []\n",
    "\n",
    "    for fig_path in required_figures:\n",
    "        try:\n",
    "            # fig_path is now already an absolute path\n",
    "            full_image_path = fig_path\n",
    "            \n",
    "            # Check if file exists\n",
    "            if not os.path.exists(full_image_path):\n",
    "                print(f\"Warning: Image file not found: {full_image_path}\")\n",
    "                continue\n",
    "\n",
    "            # Load and process image\n",
    "            with Image.open(full_image_path) as img:\n",
    "                # Convert to RGB if necessary\n",
    "                if img.mode != \"RGB\":\n",
    "                    img = img.convert(\"RGB\")\n",
    "\n",
    "                # Resize if too large (to save tokens)\n",
    "                max_size = 1024\n",
    "                if img.width > max_size or img.height > max_size:\n",
    "                    img.thumbnail((max_size, max_size), Image.Resampling.LANCZOS)\n",
    "\n",
    "                # Convert to bytes\n",
    "                buffer = io.BytesIO()\n",
    "                img.save(buffer, format=\"JPEG\", quality=85)\n",
    "                img_bytes = buffer.getvalue()\n",
    "\n",
    "                # Convert to base64\n",
    "                base64_string = base64.b64encode(img_bytes).decode(\"utf-8\")\n",
    "                data_url = f\"data:image/jpeg;base64,{base64_string}\"\n",
    "\n",
    "                image_data_urls.append(data_url)\n",
    "                valid_figures.append(fig_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {fig_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Build figure prompt with available images\n",
    "    figure_prompt = \"\"\n",
    "    if valid_figures:\n",
    "        figure_prompt = \"The following images are provided for this question:\\n\"\n",
    "        for i, fig_path in enumerate(valid_figures):\n",
    "            figure_prompt += f\"Image {i+1}: {fig_path}\\n\"\n",
    "\n",
    "    # Ensure we have valid images\n",
    "    if not image_data_urls:\n",
    "        print(f\"Warning: No valid images found for question {question_id}\")\n",
    "\n",
    "    prompt = (\n",
    "        f\"Answer this question using our own vision and reasoning and then \"\n",
    "        \"use tools to complement your reasoning. Trust your own judgement over any tools.\\\\n\\\\n\"\n",
    "        \"After using tools, you MUST provide a final reasoning and answer. Do not stop after a tool call.\\\\n\\\\n\"\n",
    "        f\"{question_data['question']}\\\\n\\\\n{figure_prompt}\\\\n\\\\n\"\n",
    "        \"Your final response should end with Final answer: <|X|> where X is the letter of your choice (A, B, C, D, E, or F).\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "\n",
    "        final_response, agent_state = run_medrax(\n",
    "            agent=agent, thread=thread, prompt=prompt, image_urls=image_data_urls\n",
    "        )\n",
    "\n",
    "        # Parse the final answer using regex\n",
    "        model_answer = \"\"\n",
    "        match = re.search(r\"<\\|([A-F])\\|>\", final_response, re.IGNORECASE)\n",
    "        if match:\n",
    "            model_answer = match.group(1).upper()\n",
    "        else:\n",
    "            print(f\"Warning: Could not parse final answer from response for question {question_id}\")\n",
    "\n",
    "        duration = time.time() - start_time\n",
    "\n",
    "        log_entry = {\n",
    "            \"case_id\": case_id,\n",
    "            \"question_id\": question_id,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"model\": model_name,\n",
    "            \"temperature\": temperature,\n",
    "            \"duration\": round(duration, 2),\n",
    "            \"usage\": \"\",\n",
    "            \"cost\": 0,\n",
    "            \"raw_response\": final_response,\n",
    "            \"model_answer\": model_answer,\n",
    "            \"correct_answer\": question_data[\"answer\"],\n",
    "            \"input\": {\n",
    "                \"messages\": prompt,\n",
    "                \"question_data\": {\n",
    "                    \"question\": question_data[\"question\"],\n",
    "                    \"explanation\": question_data[\"explanation\"],\n",
    "                    \"metadata\": question_data.get(\"metadata\", {}),\n",
    "                    \"figures\": question_data[\"images\"],\n",
    "                },\n",
    "                \"image_paths\": valid_figures,\n",
    "                \"images_processed\": len(image_data_urls),\n",
    "                \"images_found\": len(required_figures),\n",
    "            },\n",
    "            \"agent_state\": agent_state,\n",
    "        }\n",
    "\n",
    "        # Save detailed log to individual file\n",
    "        detailed_log_file = f\"{medrax_logs}/{case_id}_{question_id}_detailed.json\"\n",
    "        with open(detailed_log_file, \"w\") as f:\n",
    "            json.dump(log_entry, f, indent=2)\n",
    "\n",
    "        return final_response, model_answer\n",
    "\n",
    "    except Exception as e:\n",
    "        log_entry = {\n",
    "            \"case_id\": case_id,\n",
    "            \"question_id\": question_id,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"model\": model_name,\n",
    "            \"temperature\": temperature,\n",
    "            \"status\": \"error\",\n",
    "            \"error\": str(e),\n",
    "            \"cost\": 0,\n",
    "            \"input\": {\n",
    "                \"messages\": prompt,\n",
    "                \"question_data\": {\n",
    "                    \"question\": question_data[\"question\"],\n",
    "                    \"explanation\": question_data[\"explanation\"],\n",
    "                    \"metadata\": question_data.get(\"metadata\", {}),\n",
    "                    \"figures\": question_data[\"images\"],\n",
    "                },\n",
    "                \"image_paths\": valid_figures,\n",
    "                \"images_processed\": len(image_data_urls),\n",
    "                \"images_found\": len(required_figures),\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # Save detailed error log to individual file\n",
    "        detailed_log_file = f\"{medrax_logs}/{case_id}_{question_id}_detailed.json\"\n",
    "        with open(detailed_log_file, \"w\") as f:\n",
    "            json.dump(log_entry, f, indent=2)\n",
    "\n",
    "        print(f\"Error processing case {case_id}, question {question_id}: {str(e)}\")\n",
    "        return \"\", \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_question(question_data, tools):\n",
    "    \"\"\"Process a single question and save results to individual JSON file\"\"\"\n",
    "    try:\n",
    "        # Get a fresh agent for each question\n",
    "        agent, thread = get_agent(tools)\n",
    "        \n",
    "        case_id = question_data[\"case_id\"]\n",
    "        question_id = question_data[\"question_id\"]\n",
    "        \n",
    "        print(f\"Processing Case ID: {case_id}, Question ID: {question_id}\")\n",
    "        \n",
    "        # Create case details from the question data - simplified structure\n",
    "        case_details = {\n",
    "            \"case_id\": case_id,\n",
    "            \"images\": question_data[\"images\"],\n",
    "            \"image_source_urls\": question_data.get(\"image_source_urls\", []),\n",
    "        }\n",
    "        \n",
    "        final_response, model_answer = create_multimodal_request(\n",
    "            question_data, case_details, case_id, question_id, agent, thread\n",
    "        )\n",
    "        \n",
    "        # Create individual log file for this question\n",
    "        individual_log_file = f\"{medrax_logs}/{case_id}_{question_id}.json\"\n",
    "        \n",
    "        result = {\n",
    "            \"case_id\": case_id,\n",
    "            \"question_id\": question_id,\n",
    "            \"success\": True,\n",
    "            \"final_response\": final_response,\n",
    "            \"model_answer\": model_answer,\n",
    "            \"correct_answer\": question_data[\"answer\"],\n",
    "            \"skipped\": final_response is None or final_response == \"\"\n",
    "        }\n",
    "        \n",
    "        # Save to individual file\n",
    "        with open(individual_log_file, 'w') as f:\n",
    "            json.dump(result, f, indent=2)\n",
    "            \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Log error to individual file\n",
    "        error_log_file = f\"{medrax_logs}/{case_id}_{question_id}_ERROR.json\"\n",
    "        error_result = {\n",
    "            \"case_id\": case_id,\n",
    "            \"question_id\": question_id,\n",
    "            \"success\": False,\n",
    "            \"error\": str(e),\n",
    "            \"skipped\": True\n",
    "        }\n",
    "        \n",
    "        with open(error_log_file, 'w') as f:\n",
    "            json.dump(error_result, f, indent=2)\n",
    "            \n",
    "        print(f\"Error processing Case ID: {case_id}, Question ID: {question_id}: {str(e)}\")\n",
    "        return error_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(tools):\n",
    "    questions_data = []\n",
    "    with open(\"/home/adib/MedRAX2/chestagentbench/metadata.jsonl\", \"r\") as file:\n",
    "        for line in file:\n",
    "            question_data = json.loads(line.strip())\n",
    "\n",
    "            # Fix image paths by making them absolute\n",
    "            if \"images\" in question_data:\n",
    "                if isinstance(question_data[\"images\"], str):\n",
    "                    try:\n",
    "                        # Try to parse as JSON first\n",
    "                        images = json.loads(question_data[\"images\"])\n",
    "                        # Convert to absolute paths\n",
    "                        if isinstance(images, list):\n",
    "                            images = [\n",
    "                                os.path.join(\"/home/adib/MedRAX2/chestagentbench\", img)\n",
    "                                for img in images\n",
    "                            ]\n",
    "                        else:\n",
    "                            images = [\n",
    "                                os.path.join(\"/home/adib/MedRAX2/chestagentbench\", str(images))\n",
    "                            ]\n",
    "                        question_data[\"images\"] = images\n",
    "                    except json.JSONDecodeError:\n",
    "                        # Single image path\n",
    "                        question_data[\"images\"] = [\n",
    "                            os.path.join(\n",
    "                                \"/home/adib/MedRAX2/chestagentbench\", question_data[\"images\"]\n",
    "                            )\n",
    "                        ]\n",
    "                elif isinstance(question_data[\"images\"], list):\n",
    "                    # List of image paths\n",
    "                    question_data[\"images\"] = [\n",
    "                        os.path.join(\"/home/adib/MedRAX2/chestagentbench\", img)\n",
    "                        for img in question_data[\"images\"]\n",
    "                    ]\n",
    "\n",
    "            questions_data.append(question_data)\n",
    "\n",
    "    # Shuffle with seed 23\n",
    "    random.seed(23)\n",
    "    random.shuffle(questions_data)\n",
    "\n",
    "    # Limit to 20 questions for debugging\n",
    "    # questions_data = questions_data[:20]\n",
    "    total_questions = len(questions_data)\n",
    "\n",
    "    print(f\"Beginning benchmark evaluation for model {model_name}\\n\")\n",
    "    print(f\"Total questions to process: {total_questions}\\n\")\n",
    "    print(f\"Processing questions in parallel...\\n\")\n",
    "\n",
    "    results = []\n",
    "    batch_size = 5\n",
    "\n",
    "    # Initialize progress bar for total questions\n",
    "    with tqdm(total=total_questions, desc=\"Processing questions\", unit=\"question\") as pbar:\n",
    "        # Process questions in batches of 5\n",
    "        for i in range(0, total_questions, batch_size):\n",
    "            batch = questions_data[i : i + batch_size]\n",
    "            batch_num = i // batch_size + 1\n",
    "\n",
    "            print(\n",
    "                f\"Processing batch {batch_num}/{(total_questions + batch_size - 1) // batch_size} ({len(batch)} questions)\"\n",
    "            )\n",
    "\n",
    "            # Process batch in parallel\n",
    "            with ThreadPoolExecutor(max_workers=batch_size) as executor:\n",
    "                # Submit all questions in the batch\n",
    "                future_to_question = {\n",
    "                    executor.submit(process_single_question, question_data, tools): question_data\n",
    "                    for question_data in batch\n",
    "                }\n",
    "\n",
    "                # Collect results as they complete\n",
    "                batch_results = []\n",
    "                for future in concurrent.futures.as_completed(future_to_question):\n",
    "                    result = future.result()\n",
    "                    batch_results.append(result)\n",
    "                    pbar.update(1)  # Update progress bar after each question completes\n",
    "\n",
    "                results.extend(batch_results)\n",
    "\n",
    "            print(f\"Completed batch {batch_num}\\n\")\n",
    "\n",
    "    # Summary\n",
    "    successful = sum(1 for r in results if r[\"success\"] and not r[\"skipped\"])\n",
    "    skipped = sum(1 for r in results if r[\"skipped\"])\n",
    "    errors = sum(1 for r in results if not r[\"success\"])\n",
    "\n",
    "    print(f\"\\nBenchmark Summary:\")\n",
    "    print(f\"Total Questions: {total_questions}\")\n",
    "    print(f\"Successfully Processed: {successful}\")\n",
    "    print(f\"Skipped: {skipped}\")\n",
    "    print(f\"Errors: {errors}\")\n",
    "\n",
    "    # Save overall summary\n",
    "    summary = {\n",
    "        \"model\": model_name,\n",
    "        \"temperature\": temperature,\n",
    "        \"total_questions\": total_questions,\n",
    "        \"successful\": successful,\n",
    "        \"skipped\": skipped,\n",
    "        \"errors\": errors,\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"results\": results,\n",
    "    }\n",
    "\n",
    "    summary_file = (\n",
    "        f\"{medrax_logs}/benchmark_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    )\n",
    "    with open(summary_file, \"w\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "\n",
    "    print(f\"Summary saved to: {summary_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(tools)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medrax2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
